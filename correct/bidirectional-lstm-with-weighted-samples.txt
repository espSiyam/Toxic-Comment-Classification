{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This is my solution to [Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/overview).**","metadata":{}},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Tuple, Dict\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-23T13:49:44.211978Z","iopub.execute_input":"2021-06-23T13:49:44.212318Z","iopub.status.idle":"2021-06-23T13:49:45.117522Z","shell.execute_reply.started":"2021-06-23T13:49:44.212263Z","shell.execute_reply":"2021-06-23T13:49:45.116808Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"# a list of paths of embedding matrix files\n# Two embedding matrixes will be combined together and used in the embedding layer of the RNN.\n# They are CRAWL and GLOVE. Each of them is a collection of 300-dimension vector.\n# Each vector represents a word.\n# The coverage of words of CRAWL is different from that of GLOVE.\nEMBEDDING_FILES = [\n    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n    '../input/glove840b300dtxt/glove.840B.300d.txt'\n]\n\n# <NUM_MODELS> represents the amount of times the same model should be trained\n# Although each training is using the same RNN model, the predictions will be slightly different\n# from each other due to different initialization (He Initialization).\nNUM_MODELS = 1\n\n# amount of epoch during training, this number is mainly limited by the GPU quota during committing.\nEPOCHS = 1\n\n# batch size\nBATCH_SIZE = 256\n\n# amount of LSTM units in each LSTM layer\nLSTM_UNITS = 128\n\n# amount of unit in the dense layer\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n\n# maximum length of one comment (one sample)\nMAX_LEN = 220\n\n# column names related to identity in the training set\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\n\n# a list of all the label names (Each sample/comment corresponds to multiple labels.)\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\n# column name of the comment column\nTEXT_COLUMN = 'comment_text'\n\n# target column\nTARGET_COLUMN = 'target'\n\n# chars to remove in the comment\n# These chars are not covered by the embedding matrix. \nCHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2021-06-23T13:49:45.119887Z","iopub.execute_input":"2021-06-23T13:49:45.120327Z","iopub.status.idle":"2021-06-23T13:49:45.128212Z","shell.execute_reply.started":"2021-06-23T13:49:45.120147Z","shell.execute_reply":"2021-06-23T13:49:45.127494Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def get_coefs(word: str, *arr: str) -> (str, np.ndarray):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path: str) -> Dict[str, np.ndarray]:\n    \"\"\"Return a dict by analyzing the embedding matrix file under the path <path>.\"\"\"\n    \n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\ndef build_matrix(word_index: Dict[str, int], path: str, indexesOfWordsContainTrump: List[int]) -> np.ndarray:\n    \"\"\"Return an embedding matrix, which is ready to put into the RNN's embedding-matrix layer.\n    \n    <word_index>: Each word corresponds to a unique index. A word's vector can be found in the embedding\n        matrix using the word's index.\n    <path>: The path where the embedding matrix file is located at.\n    <indexesOfWordsContainTrump>: A list of indexes of words that contain substring \"Trump\" or \"trump\".\n    \"\"\"\n    \n    # get a word-to-vector Dict by analyzing the embedding matrix file under the path <path>\n    embedding_dict = load_embeddings(path)\n    \n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    \n    # fill the <embedding_matrix> according to <embedding_dict>\n    # If a tocken/word/string contains substring \"Trump\" or \"trump\", set the tocken/word/string's\n    # vector to be the same as Trump's.\n    # If a tocken/word cannot be found in <embedding_dict>, the tocken/word's vector is set to be zeros.\n    # Otherwise, copy a tocken/word's vector from <embedding_dict> to <embedding_matrix>.\n    for word, i in word_index.items():\n        if(i in indexesOfWordsContainTrump):\n            embedding_matrix[i] = embedding_dict['Trump']\n        else:\n            try:\n                embedding_matrix[i] = embedding_dict[word]\n            except KeyError:\n                pass\n            \n    return embedding_matrix\n\ndef build_model(embedding_matrix: np.ndarray) -> Model:\n    \"\"\"Return a RNN model, which uses bidirectional LSTM.\"\"\"\n    \n    # input layer\n    words = Input(shape=(None,))\n    \n    # embedding matrix layer-this layer should be set to be not trainable.\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    \n    # The dropout operation is used to prevent overfitting.\n    x = SpatialDropout1D(0.2)(x)\n    \n    # two bidirectional LSTM layer\n    # Since it is bidirectional, the output's size is twice the input's. \n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    # flatten the tensor by max pooling and average pooling\n    # Since it is a concatenation of two pooling layer, the output's size is twice the input's.\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    \n    # two dense layers, skip conections trick is used here to prevent gradient's vanishing.\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    \n    # two different output layers\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(len(AUX_COLUMNS), activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-23T13:49:45.129999Z","iopub.execute_input":"2021-06-23T13:49:45.130482Z","iopub.status.idle":"2021-06-23T13:49:45.148728Z","shell.execute_reply.started":"2021-06-23T13:49:45.130296Z","shell.execute_reply":"2021-06-23T13:49:45.147981Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# get the training set and test set offered in this competition\ntrain_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\ntest_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-06-23T13:49:45.150075Z","iopub.execute_input":"2021-06-23T13:49:45.150544Z","iopub.status.idle":"2021-06-23T13:50:09.204383Z","shell.execute_reply.started":"2021-06-23T13:49:45.150479Z","shell.execute_reply":"2021-06-23T13:50:09.203653Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T13:50:09.208910Z","iopub.execute_input":"2021-06-23T13:50:09.209168Z","iopub.status.idle":"2021-06-23T13:50:09.297348Z","shell.execute_reply.started":"2021-06-23T13:50:09.209122Z","shell.execute_reply":"2021-06-23T13:50:09.296735Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"      id            ...             toxicity_annotator_count\n0  59848            ...                                    4\n1  59849            ...                                    4\n\n[2 rows x 45 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>comment_text</th>\n      <th>severe_toxicity</th>\n      <th>obscene</th>\n      <th>identity_attack</th>\n      <th>insult</th>\n      <th>threat</th>\n      <th>asian</th>\n      <th>atheist</th>\n      <th>bisexual</th>\n      <th>black</th>\n      <th>buddhist</th>\n      <th>christian</th>\n      <th>female</th>\n      <th>heterosexual</th>\n      <th>hindu</th>\n      <th>homosexual_gay_or_lesbian</th>\n      <th>intellectual_or_learning_disability</th>\n      <th>jewish</th>\n      <th>latino</th>\n      <th>male</th>\n      <th>muslim</th>\n      <th>other_disability</th>\n      <th>other_gender</th>\n      <th>other_race_or_ethnicity</th>\n      <th>other_religion</th>\n      <th>other_sexual_orientation</th>\n      <th>physical_disability</th>\n      <th>psychiatric_or_mental_illness</th>\n      <th>transgender</th>\n      <th>white</th>\n      <th>created_date</th>\n      <th>publication_id</th>\n      <th>parent_id</th>\n      <th>article_id</th>\n      <th>rating</th>\n      <th>funny</th>\n      <th>wow</th>\n      <th>sad</th>\n      <th>likes</th>\n      <th>disagree</th>\n      <th>sexual_explicit</th>\n      <th>identity_annotator_count</th>\n      <th>toxicity_annotator_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>59848</td>\n      <td>0.0</td>\n      <td>This is so cool. It's like, 'would you want yo...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:41.987077+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>59849</td>\n      <td>0.0</td>\n      <td>Thank you!! This would make my life a lot less...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-09-29 10:50:42.870083+00</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>2006</td>\n      <td>rejected</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# seperate the targets and the features\nx_train = train_df[TEXT_COLUMN].astype(str)\ny_train = train_df[TARGET_COLUMN].values\ny_aux_train = train_df[AUX_COLUMNS].values\nx_test = test_df[TEXT_COLUMN].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T13:50:09.299144Z","iopub.execute_input":"2021-06-23T13:50:09.299548Z","iopub.status.idle":"2021-06-23T13:50:09.641002Z","shell.execute_reply.started":"2021-06-23T13:50:09.299384Z","shell.execute_reply":"2021-06-23T13:50:09.640141Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# change to continuous target values into discrete target values\n# There are multiple targets. Each of them contains two different classes. They are True and False.\nfor column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n    train_df[column] = np.where(train_df[column] >= 0.5, True, False)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T13:50:09.642420Z","iopub.execute_input":"2021-06-23T13:50:09.642864Z","iopub.status.idle":"2021-06-23T13:50:12.081873Z","shell.execute_reply.started":"2021-06-23T13:50:09.642684Z","shell.execute_reply":"2021-06-23T13:50:12.080850Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# One drawback of using Tokenizer is that it will change all characters to lower case.\n# But the words in both CRAW and GLOVE are case sensitive.\n# For example, \"Trump\" and \"trump\" are represented by different vectors in CRAWL or GLOVE.\n# But Tokenizer will change \"Trump\" into \"trump\".\ntokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T13:50:12.083368Z","iopub.execute_input":"2021-06-23T13:50:12.083712Z","iopub.status.idle":"2021-06-23T13:50:12.089161Z","shell.execute_reply.started":"2021-06-23T13:50:12.083660Z","shell.execute_reply":"2021-06-23T13:50:12.088269Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# A word-to-index Dict will be generated internally after analyzing the train set and the test set.\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","metadata":{"execution":{"iopub.status.busy":"2021-06-23T13:50:12.090748Z","iopub.execute_input":"2021-06-23T13:50:12.091379Z","iopub.status.idle":"2021-06-23T13:52:00.019983Z","shell.execute_reply.started":"2021-06-23T13:50:12.091322Z","shell.execute_reply":"2021-06-23T13:52:00.019083Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Replace all the words/tokens in train/test set by the corresponding index according to the\n# internal word-to-index Dict.\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T13:52:00.021311Z","iopub.execute_input":"2021-06-23T13:52:00.021609Z","iopub.status.idle":"2021-06-23T13:53:37.865364Z","shell.execute_reply.started":"2021-06-23T13:52:00.021564Z","shell.execute_reply":"2021-06-23T13:53:37.864623Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# make the length of all the sequencess the same\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T13:53:37.866622Z","iopub.execute_input":"2021-06-23T13:53:37.866900Z","iopub.status.idle":"2021-06-23T13:54:05.881868Z","shell.execute_reply.started":"2021-06-23T13:53:37.866858Z","shell.execute_reply":"2021-06-23T13:54:05.881019Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# assign different weights to different samples according to their labels\n# This is because different groups have different effect on the evaluation metric.\n# Another reason is that the evaluation metric is too complicated to be directly used during optimization.\n# The following specific weight assignment is decided after many tries.\nsample_weights = np.ones(len(x_train), dtype=np.float32)\nsample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\nsample_weights += train_df[TARGET_COLUMN] & (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\nsample_weights += (~train_df[TARGET_COLUMN]) & train_df[IDENTITY_COLUMNS].sum(axis=1) \nsample_weights += (~train_df[TARGET_COLUMN] & train_df['homosexual_gay_or_lesbian'] + 0) * 5\nsample_weights += (~train_df[TARGET_COLUMN] & train_df['black'] + 0) * 5\nsample_weights += (~train_df[TARGET_COLUMN] & train_df['white'] + 0) * 5\nsample_weights += (~train_df[TARGET_COLUMN] & train_df['muslim'] + 0) * 1\nsample_weights += (~train_df[TARGET_COLUMN] & train_df['jewish'] + 0) * 1\nsample_weights /= sample_weights.mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T13:54:05.883350Z","iopub.execute_input":"2021-06-23T13:54:05.883837Z","iopub.status.idle":"2021-06-23T13:54:07.315172Z","shell.execute_reply.started":"2021-06-23T13:54:05.883789Z","shell.execute_reply":"2021-06-23T13:54:07.314436Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"indexesOfWordsContainTrump = []\n\n# find out all the indexes of the words that contain substring \"Trump\" or \"trump\"\nfor word, index in tokenizer.word_index.items():\n    if(('trump' in word) or ('Trump' in word)):\n        indexesOfWordsContainTrump.append(index)\n\n# The final embedding matrix is a concatenation of CRAWL embedding matrix and GLOVE embedding matrix.\n# So each word is represented by a 600-d vector.\n# In the final matrix, the words that contain substring \"Trump\" or \"trump\" are replaced by \"Trump\".\n# This is found to be able to enhance the model performance by EDA(exploratory data analysis).\n# The reason behind this is that strings like \"Trump\" and \"trumpist\" are related to toxicity,\n# but they are covered neither in CRAWL nor GLOVE.\nembedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, filePath, indexesOfWordsContainTrump) for filePath in EMBEDDING_FILES], axis=-1)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T13:54:07.316489Z","iopub.execute_input":"2021-06-23T13:54:07.316770Z","iopub.status.idle":"2021-06-23T14:02:21.443041Z","shell.execute_reply.started":"2021-06-23T13:54:07.316726Z","shell.execute_reply":"2021-06-23T14:02:21.442286Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# release memory space by deleting variables that are no longer useful\ndel train_df\ndel tokenizer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:02:21.444431Z","iopub.execute_input":"2021-06-23T14:02:21.444713Z","iopub.status.idle":"2021-06-23T14:02:21.562970Z","shell.execute_reply.started":"2021-06-23T14:02:21.444670Z","shell.execute_reply":"2021-06-23T14:02:21.562004Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"26"},"metadata":{}}]},{"cell_type":"code","source":"# <checkpoint_predictions> is a list of predictions generated after each epoch.\ncheckpoint_predictions = []\n# <weights> is a list of weights corresponding to <checkpoint_predictions>.\nweights = []\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix)\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            x_train,\n            [y_train, y_aux_train],\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=1,\n            sample_weight=[sample_weights.values, np.ones_like(sample_weights)],\n            callbacks=[\n                LearningRateScheduler(lambda _: 1e-3 * (0.6 ** global_epoch))\n            ]\n        )\n        \n        # record predictions after each epoch\n        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n        # Since the predictions tend to be more accurate after more epochs,\n        # the weights is set to grow exponetially.\n        weights.append(2 ** global_epoch)\n\n# get the weighted average of the predictions. The average operation can help prevent overfitting.\npredictions = np.average(checkpoint_predictions, weights=weights, axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:02:21.564342Z","iopub.execute_input":"2021-06-23T14:02:21.564842Z","iopub.status.idle":"2021-06-23T14:17:55.489567Z","shell.execute_reply.started":"2021-06-23T14:02:21.564673Z","shell.execute_reply":"2021-06-23T14:17:55.488446Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/1\n1804874/1804874 [==============================] - 912s 505us/step - loss: 0.4077 - dense_3_loss: 0.3027 - dense_4_loss: 0.1050\n","output_type":"stream"}]},{"cell_type":"code","source":"# output the averaged predictions to a file for submission\nsubmission = pd.DataFrame.from_dict({\n    'id': test_df.id,\n    'prediction': predictions\n})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:17:55.490997Z","iopub.execute_input":"2021-06-23T14:17:55.491610Z","iopub.status.idle":"2021-06-23T14:17:56.292360Z","shell.execute_reply.started":"2021-06-23T14:17:55.491528Z","shell.execute_reply":"2021-06-23T14:17:56.291509Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:22:00.387327Z","iopub.execute_input":"2021-06-23T14:22:00.387651Z","iopub.status.idle":"2021-06-23T14:22:00.848252Z","shell.execute_reply.started":"2021-06-23T14:22:00.387598Z","shell.execute_reply":"2021-06-23T14:22:00.847493Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2021-06-23T14:17:56.293665Z","iopub.execute_input":"2021-06-23T14:17:56.293953Z","iopub.status.idle":"2021-06-23T14:17:56.316458Z","shell.execute_reply.started":"2021-06-23T14:17:56.293906Z","shell.execute_reply":"2021-06-23T14:17:56.315345Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"            id  prediction\n0      7097320    0.028603\n1      7097321    0.047399\n2      7097322    0.161565\n3      7097323    0.053203\n4      7097324    0.014609\n5      7097325    0.009782\n6      7097326    0.736632\n7      7097327    0.340901\n8      7097328    0.005534\n9      7097329    0.030645\n10     7097330    0.018161\n11     7097331    0.267441\n12     7097332    0.079826\n13     7097333    0.030476\n14     7097334    0.192427\n15     7097335    0.005699\n16     7097336    0.089979\n17     7097337    0.006340\n18     7097338    0.012314\n19     7097339    0.553554\n20     7097340    0.009371\n21     7097341    0.163997\n22     7097342    0.022256\n23     7097343    0.006997\n24     7097344    0.031281\n25     7097345    0.028599\n26     7097346    0.424164\n27     7097347    0.109976\n28     7097348    0.046886\n29     7097349    0.012931\n...        ...         ...\n97290  7194610    0.050780\n97291  7194611    0.106835\n97292  7194612    0.010736\n97293  7194613    0.032121\n97294  7194614    0.135022\n97295  7194615    0.050151\n97296  7194616    0.009719\n97297  7194617    0.129103\n97298  7194618    0.004949\n97299  7194619    0.135788\n97300  7194620    0.063357\n97301  7194621    0.204282\n97302  7194622    0.014047\n97303  7194623    0.004600\n97304  7194624    0.682926\n97305  7194625    0.856476\n97306  7194626    0.054750\n97307  7194627    0.050740\n97308  7194628    0.085812\n97309  7194629    0.045788\n97310  7194630    0.005916\n97311  7194631    0.073541\n97312  7194632    0.154142\n97313  7194633    0.006025\n97314  7194634    0.010709\n97315  7194635    0.046151\n97316  7194636    0.090795\n97317  7194637    0.465942\n97318  7194638    0.282123\n97319  7194639    0.029015\n\n[97320 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7097320</td>\n      <td>0.028603</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7097321</td>\n      <td>0.047399</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7097322</td>\n      <td>0.161565</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7097323</td>\n      <td>0.053203</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7097324</td>\n      <td>0.014609</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>7097325</td>\n      <td>0.009782</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7097326</td>\n      <td>0.736632</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7097327</td>\n      <td>0.340901</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>7097328</td>\n      <td>0.005534</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>7097329</td>\n      <td>0.030645</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>7097330</td>\n      <td>0.018161</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>7097331</td>\n      <td>0.267441</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>7097332</td>\n      <td>0.079826</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>7097333</td>\n      <td>0.030476</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>7097334</td>\n      <td>0.192427</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>7097335</td>\n      <td>0.005699</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>7097336</td>\n      <td>0.089979</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>7097337</td>\n      <td>0.006340</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>7097338</td>\n      <td>0.012314</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>7097339</td>\n      <td>0.553554</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>7097340</td>\n      <td>0.009371</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>7097341</td>\n      <td>0.163997</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>7097342</td>\n      <td>0.022256</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>7097343</td>\n      <td>0.006997</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>7097344</td>\n      <td>0.031281</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>7097345</td>\n      <td>0.028599</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>7097346</td>\n      <td>0.424164</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>7097347</td>\n      <td>0.109976</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>7097348</td>\n      <td>0.046886</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>7097349</td>\n      <td>0.012931</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>97290</th>\n      <td>7194610</td>\n      <td>0.050780</td>\n    </tr>\n    <tr>\n      <th>97291</th>\n      <td>7194611</td>\n      <td>0.106835</td>\n    </tr>\n    <tr>\n      <th>97292</th>\n      <td>7194612</td>\n      <td>0.010736</td>\n    </tr>\n    <tr>\n      <th>97293</th>\n      <td>7194613</td>\n      <td>0.032121</td>\n    </tr>\n    <tr>\n      <th>97294</th>\n      <td>7194614</td>\n      <td>0.135022</td>\n    </tr>\n    <tr>\n      <th>97295</th>\n      <td>7194615</td>\n      <td>0.050151</td>\n    </tr>\n    <tr>\n      <th>97296</th>\n      <td>7194616</td>\n      <td>0.009719</td>\n    </tr>\n    <tr>\n      <th>97297</th>\n      <td>7194617</td>\n      <td>0.129103</td>\n    </tr>\n    <tr>\n      <th>97298</th>\n      <td>7194618</td>\n      <td>0.004949</td>\n    </tr>\n    <tr>\n      <th>97299</th>\n      <td>7194619</td>\n      <td>0.135788</td>\n    </tr>\n    <tr>\n      <th>97300</th>\n      <td>7194620</td>\n      <td>0.063357</td>\n    </tr>\n    <tr>\n      <th>97301</th>\n      <td>7194621</td>\n      <td>0.204282</td>\n    </tr>\n    <tr>\n      <th>97302</th>\n      <td>7194622</td>\n      <td>0.014047</td>\n    </tr>\n    <tr>\n      <th>97303</th>\n      <td>7194623</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <th>97304</th>\n      <td>7194624</td>\n      <td>0.682926</td>\n    </tr>\n    <tr>\n      <th>97305</th>\n      <td>7194625</td>\n      <td>0.856476</td>\n    </tr>\n    <tr>\n      <th>97306</th>\n      <td>7194626</td>\n      <td>0.054750</td>\n    </tr>\n    <tr>\n      <th>97307</th>\n      <td>7194627</td>\n      <td>0.050740</td>\n    </tr>\n    <tr>\n      <th>97308</th>\n      <td>7194628</td>\n      <td>0.085812</td>\n    </tr>\n    <tr>\n      <th>97309</th>\n      <td>7194629</td>\n      <td>0.045788</td>\n    </tr>\n    <tr>\n      <th>97310</th>\n      <td>7194630</td>\n      <td>0.005916</td>\n    </tr>\n    <tr>\n      <th>97311</th>\n      <td>7194631</td>\n      <td>0.073541</td>\n    </tr>\n    <tr>\n      <th>97312</th>\n      <td>7194632</td>\n      <td>0.154142</td>\n    </tr>\n    <tr>\n      <th>97313</th>\n      <td>7194633</td>\n      <td>0.006025</td>\n    </tr>\n    <tr>\n      <th>97314</th>\n      <td>7194634</td>\n      <td>0.010709</td>\n    </tr>\n    <tr>\n      <th>97315</th>\n      <td>7194635</td>\n      <td>0.046151</td>\n    </tr>\n    <tr>\n      <th>97316</th>\n      <td>7194636</td>\n      <td>0.090795</td>\n    </tr>\n    <tr>\n      <th>97317</th>\n      <td>7194637</td>\n      <td>0.465942</td>\n    </tr>\n    <tr>\n      <th>97318</th>\n      <td>7194638</td>\n      <td>0.282123</td>\n    </tr>\n    <tr>\n      <th>97319</th>\n      <td>7194639</td>\n      <td>0.029015</td>\n    </tr>\n  </tbody>\n</table>\n<p>97320 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}